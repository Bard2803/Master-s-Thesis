# Master's-Thesis

## Abstract

There are many studies that compare the performance of different Continual Learning strategies, but they mainly focus on their final accuracy. Here, the collation is done with a focus on resource efficiency. Four different Continual Learning strategies models are trained on the CORe50 dataset with a fixed number of epochs in order to compare their resource efficiency. They are selected as representatives of each of the Continual Learning strategies categories: regularization, rehearsal, generative replay and architectural. They are, respectively: Elastic Weights Consolidation (EWC), Gradient Episodic Memory (GEM), Generative Replay and Copy Weights with Regularization (CWR*). The Cumulative and the Na誰ve approaches set the upper and lower boundary for the accuracy. The most resource demanding Continual Learning strategy is GEM, which is even higher than the Cumulative approach (being the second). Then there are Generative Replay, next EWC. As the least resource demanding are CWR* and Na誰ve, which are very similar and hard to distinguish. In some resources CWR* is more efficient while in some Na誰ve approach. The accuracy which is measured on a separate test set, which is kept constant throughout all experiences, is correlated with the resource utilization. The Continual strategy which is the most resource demanding has the best accuracy (GEM). Only, the Cumulative approach has better accuracy, although it uses less computational resources for training. Then the order is maintained, ending with Na誰ve approach having the worst accuracy. The gap between Cumulative approach accuracy and GEM accuracy is significant. What proves that Continual Learning strategies still have worse performance than classical approach. 

Keywords: Continual Learning, comparison, resource efficiency, strategies, metrics, GPU, CPU, RAM, power usage, energy usage, accuracy.
